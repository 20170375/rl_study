## 3장 강화학습 기초2: 그리드월드와 다이내믹 프로그래밍
**정책 이터레이션**: 다이내믹 프로그래밍으로 **벨만 기대 방정식**을 푸는 것

**가치 이터레이션**: 다이내믹 프로그래밍으로 **벨만 최적 방정식**을 푸는 것

<br>

### 다이내믹 프로그래밍과 그리드월드
순차적 행동 결정문제: 
벨만 방정식을 통해 순차적 행동 결정 문제를 푸는 방법을 정리하면 다음과 같습니다.
  1. 순차적 행동 문제를 MDP로 전환한다.
  2. 가치함수를 벨만 방정식으로 반복적으로 계산한다.
  3. 최적 가치함수와 최적 정책을 찾는다.

**다이내믹 프로그래밍**
- 큰 문제 안에 작은 문제들이 중첩된 경우, 작은 문제로 쪼개서 풀겠다는 것
- 한번에 v<sub>π</sub>(s)를 구하는 것이 아니라 여러 번에 나눠서 구하는 것

<br>

### 다이내믹 프로그래밍 1: 정책 이터레이션
- **정책 이터레이션**: 현재의 정책을 "**평가**"하고 더 나은 정책으로 "**발전**"시키기를 무한히 반복하면 최적 정책으로 수렴한다.
- **정책 평가**: 가치함수를 근거로 정책이 얼마나 좋은지 판단한다.

  vπ(s) = Eπ[rt+1 + γ vπ(St+1) | St = s]
    
  주변 상태의 가치함수와 한 타임스텝의 보상만 고려해서 현재 상태의 다음 가치함수를 계산한다.
  
  계산 결과는 참 가치함수가 아니지만, 이러한 계산을 여러 번 반복한다면 참 값으로 수렴한다.
- **정책 발전**: 각 상태에 대해 어떤 행동을 하는 것이 좋은지는 큐함수를 사용하면 알 수 있다.

  q<sub>π</sub>(s, a) = E<sub>π</sub>[r<sub>t+1</sub> + γ q<sub>π</sub>(S<sub>t+1</sub>, A<sub>t+1</sub>) | S<sub>t</sub> = s, A<sub>t</sub> = a]
   
- **탐욕 정책 발전**: 상태 s에서 선택 가능한 행동의 q<sub>π</sub>(s, a)를 비교하고 그중에서 가장 큰 큐함수를 가지는 행동을 선택한다.

<br>

### 다이내믹 프로그래밍 2: 가치 이터레이션
- **가치 이터레이션**: 가치함수가 최적이라고 가정하고 반복적으로 가치함수를 발전시켜서 최적 가치함수에 도달.
- **벨만 최적 방정식**: v*<sub>π</sub>(s) = max<sub>a</sub> E<sub>π</sub>[r<sub>t+1</sub> + γ v*(S<sub>t+1</sub>) | S<sub>t</sub> = s]

  현재 상태에서 가능한 최고의 선택을 하기 때문에 계산 가능한 형태로 변환하면 다음과 같다.
  
  v<sub>k+1</sub>(s) = max<sub>a∈A</sub> ( r(s, a) + γ v<sub>k</sub>(S') )

반복적으로 계산하여 최적 정책에 대한 참 가치함수, 즉 최적 가치함수를 찾는다.

<br>

### 다이내믹 프로그래밍의 한계
1. **계산 복잡도**

    문제의 규모가 커진다면 계산만으로는 풀어내기에 한계가 있다.
  
2. **차원의 저주**
 
    상태의 차원이 늘어나면 상태의 수가 지수적으로 증가한다.
  
3. **환경에 대한 완벽한 정보가 필요**

    보통은 "환경의 모델" (보상과 상태 변환 확률)을 정확히 알 수 없다.

**강화학습**: 모델 없이 환경과의 상호작용을 통해 입력과 출력 사이의 관계를 학습한다.
