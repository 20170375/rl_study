## 4장 강화학습 기초3: 그리드월드와 큐러닝

### 강화학습과 정책 평가 1: 몬테카를로 예측
**몬테카를로 근사**: 무작위로 무엇인가를 해서 원래의 값을 추정하는 것

**몬테카를로 예측**: 샘플링을 통해 얻은 샘플의 평균으로 참 가치함수의 값을 추정한다.

- **샘플링**: 에이전트가 한 번 환경에서 에피소드를 진행하는 것

- **몬테카를로 예측에서 가치함수의 업데이트 식**: 

  V(s) ← V(s) + 1/n (G(s) - V(s))

  몬테카를로 예측에서 에이전트는 이 업데이트 식을 통해 에피소드 동안 경험했던 모든 상태에 대해 가치함수를 업데이트한다.

<br>

### 강화학습과 정책 평가 2: 시간차 예측
몬테카를로 예측은 실시간이 아니기 대문에 가치함수를 업데이트하기 위해서는 에피소드가 끝날 때까지 기다려야 한다.

**시간차 예측**<sup>temporal-Difference Prediction</sup>
- 타임스텝마다 가치함수를 업데이트
- 몬테카를로 예측보다 초기 가치함수 값에 따라 예측 정확도가 많이 달라진다는 단점 존재
- 시간차 예측에서 가치함수의 업데이트 식:

  V(S<sub>t</sub>) ← V(S<sub>t</sub>) + α (R<sub>t+1</sub> + γ V(S<sub>t+1</sub>) - V(s<sub>t</sub>))

<br>
  
### 강화학습 알고리즘 1: 살사
**시간차 제어**: 시간차 예측 + 탐욕 정책

가치 이터레이션과 같이, 별도의 정책을 두지 않고 에이전트가 현재 상태에서 가장 큰 가치를 지니는 행동을 선택하는 탐욕 정책을 사용한다

탐욕 정책에서 환경의 모델을 알지 못하기 때문에, 다음 상태의 가치함수를 보고 판단하지 않고, 현재 상태의 큐함수를 보고 판단한다.

- **큐함수를 사용한 탐욕 정책**:
  
  π(s) = argmax<sub>a∈A</sub> Q(s, a)

- **시간차 제어에서 큐함수의 업데이트 식**:
  
  Q(S<sub>t</sub>, A<sub>t</sub>) ← Q(S<sub>t</sub>, A<sub>t</sub>) + α (R<sub>t+1</sub> + γ Q(S<sub>t+1</sub>, A<sub>t+1</sub>) - Q(S<sub>t</sub>, A<sub>t</sub>))

- [S<sub>t</sub>, A<sub>t</sub>, R<sub>t+1</sub>, S<sub>t+1</sub>, A<sub>t+1</sub>]을 하나의 샘플로 사용하기 때문에 시간체 제어를 다른 말로 살사<sup>SARSA</sup>라고 부릅니다.

**ε-탐욕 정책**:

ε만큼의 확률로 탐욕적이지 않은 행동을 선택하는 탐욕 정책

에이전트가 충분히 다양한 경험을 하도록 하기 위한 탐욕 정책

<img src=https://user-images.githubusercontent.com/62216628/169803827-a8780510-cd75-4a60-bf19-191bc002ee8a.png width=300px/>

