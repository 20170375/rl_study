## 6장 강화학습 심화 2: 카트풀
### 알고리즘 1: DQN
**경험 리플레이**

- 에이전트가 환경에서 탐험하며 얻는 샘플 (s, a, r, s')을 메모리에 저장하여, 학습시 여러 개의 샘플을 무작위로[^1] 뽑아서 뽑은 샘플에 대해 인공신경망을 업데이트한다.

- 경험 리플레이 자체가 지금 에이전트가 경험하고 있는 상황<sup>현재 정책으로부터 발생한 상황</sup>이 아닌 다양한 과거의 상황<sup>이전의 정책으로부터 발생한 상황</sup>으로부터 학습하기 때문에 **오프폴리시 알고리즘**이 적합하다.

**DQN**

- **DQN**에서는 **타깃신경망**<sup>target network</sup>을 따로 사용하여 정답에 해당하는 값을 구하고 다른 인공신경망을 계속 학습시키며, 타깃신경망은 일정한 시간 간격마다 그 인공신경망으로 업데이트한다.

- **타깃 네트워크를 이용한 DQN 오류함수의 정의**

  MSE = (정답 - 예측)<sup>2</sup> = (R<sub>t+1</sub> + γ max<sub>a'</sub> Q(S<sub>t+1</sub>, a', θ<sup>-</sup>) - Q(S<sub>t</sub>, A<sub>t</sub>, θ))<sup>2</sup>[^2]

<br>

## 알고리즘 2: 액터-크리틱
액터-크리틱은 REINFORCE 알고리즘의 단점을 해결하기 위해 다이내믹 프로그래밍의 정책 이터레이션의 구조를 사용했다.

REINFORCE 알고리즘에서는 정책신경망 업데이트 시 큐함수 대신 반환값을 사용했지만, 
액터-크리틱에서는 인공신경망을 두 개 만들어서 하나는 정책을 근사하고 다른 하나는 큐함수를 근사하는 방법을 사용한다.

큐함수를 근사하는 인공신경망을 **가치신경망**이라고 하며, 그 역할은 정책을 평가하는 것이기 때문에 **크리틱**<sup>Critic</sup>이라는 이름이 붙었다.

<img src=https://user-images.githubusercontent.com/62216628/170490272-4b3c7c92-7c0e-4403-9ad0-1cccae7acc42.png width=300px/>

- **액터-크리틱의 업데이트 식**

  θ<sub>t+1</sub> ≈ θ<sub>t</sub> + α[∇<sub>θ</sub> log π<sub>θ</sub>(a | s) Q<sub>w</sub>(s,a)][^3]

- **어드벤티지 함수** 

  액터-크리틱에서는 오류함수의 큐함수에 대한 변화 정도를 줄여주기 위해, 가치함수를 베이스라인으로 큐함수에서 빼준 **어드벤티지 함수**를 사용한다.

  A(S<sub>t</sub>, A<sub>t</sub>) = Q<sub>w</sub>(S<sub>t</sub>, A<sub>t</sub>)  - V<sub>v</sub>(S<sub>t</sub>) 

  δ<sub>v</sub> = R<sub>t+1</sub> + γ V<sub>v</sub>(S<sub>t+1</sub>) - V<sub>v</sub>(S<sub>t</sub>)

- **어드벤티지 함수를 사용한 액터-크리틱의 업데이트 식**

  θ<sub>t+1</sub> ≈ θ<sub>t</sub> + α[∇<sub>θ</sub> log π<sub>θ</sub>(a | s)  δ<sub>v</sub>]

- **가치신경망의 업데이트 식**

  가치신경망은 가치함수를 근사하는 인공신경망이므로 이 또한 학습이 필요하다. 아래의 시간차 오류 (MSE함수)를 최소화하는 방향으로 업데이트를 진행한다.

  MSE = (정답 - 예측)<sup>2</sup> = (R<sub>t+1</sub> + γ V<sub>v</sub>(S<sub>t+1</sub>) - V<sub>v</sub>(S<sub>t</sub>))<sup>2</sup>

<img src=https://user-images.githubusercontent.com/62216628/170490857-dbb3cdb4-84a0-486a-b2a3-4c45d864c51e.png width=300px/>



[^1]: 리플레이 메모리를 사용할 경우 학습에 사용하는 샘플들은 서로 시간적인 상관관계가 없으므로 인공신경망이 나쁜 상황으로 편향되지 않는다.
[^2]: 타깃신경망의 매개변수는 θ<sup>-</sup>로 인공신경망의 매개변수는 θ로 표현
[^3]: 가치신경망의 가중치 w
