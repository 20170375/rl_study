## 6장 강화학습 심화 2: 카트풀
### 알고리즘 1: DQN
**경험 리플레이**

- 에이전트가 환경에서 탐험하며 얻는 샘플 (s, a, r, s')을 메모리에 저장하여, 학습시 여러 개의 샘플을 무작위로[^1] 뽑아서 뽑은 샘플에 대해 인공신경망을 업데이트한다.

- 경험 리플레이 자체가 지금 에이전트가 경험하고 있는 상황<sup>현재 정책으로부터 발생한 상황</sup>이 아닌 다양한 과거의 상황<sup>이전의 정책으로부터 발생한 상황</sup>으로부터 학습하기 때문에 **오프폴리시 알고리즘**이 적합하다.

**DQN**

- **DQN**에서는 **타깃신경망**<sup>target network</sup>을 따로 사용하여 정답에 해당하는 값을 구하고 다른 인공신경망을 계속 학습시키며, 타깃신경망은 일정한 시간 간격마다 그 인공신경망으로 업데이트한다.

- **타깃 네트워크를 이용한 DQN 오류함수의 정의**

  MSE = (정답 - 예측)<sup>2</sup> = (R<sub>t+1</sub> + γ max<sub>a'</sub> Q(S<sub>t+1</sub>, a', θ<sup>-</sup>) - Q(S<sub>t</sub>, A<sub>t</sub>, θ))<sup>2</sup>[^2]

<br>

## 알고리즘 2: 액터-크리틱

[^1]: 리플레이 메모리를 사용할 경우 학습에 사용하는 샘플들은 서로 시간적인 상관관계가 없으므로 인공신경망이 나쁜 상황으로 편향되지 않는다.
[^2]: 타깃신경망의 매개변수는 θ<sup>-</sup>로 인공신경망의 매개변수는 θ로 표현




