## 5장 강화학습 심화 1: 그리드월드와 근사함수
**근사함수**
- 기존의 데이터를 매개변수를 통해 근사하는 함수

- 기존의 문제를 해결하기 위해, 테이블이 아닌 근사함수로 가치함수를 표현한다.

**딥살사**
- 큐함수를 인공신경망으로 근사한 살사 알고리즘

- 딥살사의 오차함수:

  MSE = (정답 - 예측)<sup>2</sup> = (R<sub>t+1</sub> + γ Q<sub>θ</sub>(S<sub>t+1</sub>, A<sub>t+1</sub>) - Q<sub>θ</sub>(S<sub>t</sub>, A<sub>t</sub>))<sup>2</sup>[^1]

[^1]: Q<sub>θ</sub>는 매개변수가 θ인 인공신경망을 통해 표현한 큐함수

<br>

### **폴리시 그레이디언트**
**정책 기반 강화학습**<sup>Policy-based Reinforcement Learning</sup>은 인공신경망이 정책을 근사 (←→ 가치 기반 강화학습인 딥살사는 인공신경망이 큐함수를 근사)

목표함수의 경사상승법[^2]을 따라서 근사된 정책을 업데이트하는 방식을 **폴리시 그레이디언트**라고 한다.

[^2]: 누적 보상을 최대로 하기 위해, 딥살사와는 다르게 목표가 오류함수를 최소화하는 것이 아니라 목표함수를 최대화하는 것이므로 경사상승법을 적용한다.

- **폴리시 그레이디언트의 업데이트 식**

  θ<sub>t+1</sub> = θ<sub>t</sub> + α∇<sub>θ</sub> J(θ) ≈ θ<sub>t</sub> + α[∇<sub>θ</sub> log π<sub>θ</sub>(a | s) q<sub>π</sub>(s,a)]

  위 식에서 목표함수의 미분값 α∇<sub>θ</sub> J(θ)를 근사하는 방법 중 큐함수를 반환값 G<sub>t</sub>로 대체하는 것이 **REINFORCE 알고리즘**입니다.
  
  REINFORCE 알고리즘은 한 에피소드가 끝나면 지나온 상태에 대해 실제로 얻은 보상으로 학습하는 폴리시 그레이디언트라고 할 수 있습니다.
